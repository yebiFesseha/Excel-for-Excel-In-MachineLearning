# Excel-for-Excel-In-MachineLearning.
01: LinReg: Basic linear regression ML, I have used list comprhension in a number of places to illustrate the beauty of python.

02: Logistic Regression: Run a Logistic Regression on soccer matches data, using Column Transformers and Pipeline. Used OneHotEncoding and svm.SVR for modelling and got 63% accuracy

03: Random Forest Classifier: random forest model for Behavioral Risk Factor Surveillance System data, work in progress. This is just the beginning of the journey.

04: Support Vector Machine: run a svm.SVC with 4 kernels and svm.SVR with linear kernel and a bunch of thresholds.

05: Gradient Descent: an attempt to understand Gradient Descent

06: Categorical: Here I am showing how a categorical data can be transformed and encoded before running an ML algoithm. The data is ficitious with a number of categorical columns. Support Vector Machine with SVC, Linear Regression and Decision Tree Classifier have been used for prediction and their accuracy has been measured against the test data.

07: Heart attack predictor

08: K-Fold Cross Validation: I have run a number of models with k-fold CV and used pipeline.

09: First NLP project using NLTK and Regex: I used a Twitter comment data I got from Kaggle. The goal here is to get familiar with the available NLP libraries and create a Word Cloud.

10: How to execute SQLite3 commands: SQL is a critical skill for a data analyst or scientist to have. Here i have demonstrated basic execution of sqlite commands using python/pandas. I have created a fictious database with 3 tables. Populated two of them with data and removed or dropped the third one for demonstration purposes. Finally, I used pandas to read a table from a sqlite3 database and display it on a dataframe.

11: SQL based data analysis: some functions on google Colab don't work such as Rank, Dense Rank. So far I am not able to resolve the issue.

12: Unsupervised ML KMeans Clustering: I used the world Happiness index report from Kaggle as a toy data to practice clustering in Unsupervised machine learning algorithm, specifically KMeans algorithm. In addition to KMeans algorithm i have shown a number of ML techniques and processes, Labelled countries data, preprocessed the data,
using elbow method found the optimum number of clusters, Worked out the optimum number of components using explained_variance_ratio, hence the number of optimum dimensions when applying PCA dimentional recuction technique and finally used plotly to visualise the results.

13: Pipeline, Column Transformers, CV, Logistic Regression: In this case I have modelled the probability of churn from an imaginary cellular company. The challenge is how to model both categorical and numerical data. I have used Logistic Regression modelling on scaled numerical and encoded categorical data using column transformers and pipelining. The result shows an improvement of about 3% churn prediction accuracy from about 76% to 79%.
